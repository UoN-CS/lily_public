{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UoN-CS/lily_public/blob/main/COMP3002%20Task%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9eaf3cd",
      "metadata": {
        "id": "d9eaf3cd"
      },
      "source": [
        "# COMP3002 Big Data and Cloud Project\n",
        "## Task 1\n",
        "\n",
        "This notebook describes the tasks that you must complete for the first task.  You should complete the work in this notebook and ensure that you regularly commit it to your GitHub classroom.  You can choose to include additional python .py files if you wish to create some helper functions to keep this notebook clean.  Make sure they are committed to the GitHub repository too.\n",
        "\n",
        "### Scenario\n",
        "\n",
        "You are provided with a small sample dataset of Amazon Review Data.  This notebook talks you through the process of loading that data into Spark SQL and asks you to analyse that data.  On the Block Release day on 18th November (Open Cohort) and 20th November (Ford Cohort) you will have access to a larger dataset hosted in the Cloud.  Much of the day will be spent moving your solutions to the cloud, and answering additional questions which will be set on the day.\n",
        "\n",
        "If you do not finish everything during Block Release.  You will have additionl time to reflect on the experience and finalise your code before final submission on 26th November.\n",
        "\n",
        "Amazon Review Data was downloaded from [here](https://jmcauley.ucsd.edu/data/amazon/) but a small sample is provided with this assignment.\n",
        "\n",
        "### Learning Outcomes\n",
        "\n",
        "Remember that the primary aim with this task is not to get the \"correct\" answer, but for you to use the time to become confident with some basic Big Data processing.\n",
        "\n",
        "* **LO1** Understand the principles that allow the processing of big data sets.\n",
        "\n",
        "* **LO3** Understand the limitations of big data technologies for distributed processing.\n",
        "\n",
        "* **LO3** Demonstrate practical skills required to implement big-data solutions using modern large-scale data and compute infrastructures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59ff4c94",
      "metadata": {
        "id": "59ff4c94"
      },
      "source": [
        "### Assessment\n",
        "\n",
        "Assessment follows a similar approach to that used previously on the programme. This small task attracts up to a grade C. The second task to be released later this term will allow you to stretch to higher grades.\n",
        "\n",
        "---\n",
        "\n",
        "### Grading Criteria\n",
        "\n",
        "#### **Grade C (50)**\n",
        "In addition to the requirements for D-grade, the work should:\n",
        "*   Demonstrate the ability to implement a solution to the challenge tasks posed during the block release day using the Spark Cluster.\n",
        "\n",
        "If the solution is not complete, a C-grade may still be awarded if a strong narrative is provided to explain where further work is needed and what the next steps would be.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Grade D (40)**\n",
        "As this is the passing grade for the project, you must achieve all the learning outcomes.\n",
        "\n",
        "The work should meet the following minimum criteria:\n",
        "*   Work should be a Jupyter notebook submitted via Github Classrooms with accompanying helper .py files that are free from errors and execute successfully.\n",
        "*   The notebook demonstrates that the apprentice can:\n",
        "    1.  Connect to a spark context.\n",
        "    2.  Transmit data to Spark.\n",
        "    3.  Execute remote transformations and actions on Spark.\n",
        "    4.  Retrieve outputs and present them in a suitable manner.\n",
        "*   Provide acceptable answers to questions posed in the task template.\n",
        "\n",
        "The work may be limited in that:\n",
        "*   It may only run on a single machine via PySpark.\n",
        "*   It may not demonstrate an attempt at the challenge tasks posed during the block release day.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Grade E (30)**\n",
        "Learning outcomes not met at threshold level, but with additional work a pass could be achieved. This may mean that code does not run, or solutions are that achieve the brief but without successfully using the Spark infrastructure.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Grade F (0-29)**\n",
        "Learning outcomes not met at threshold level, but with additional work a pass could be achieved. This may mean that code does not run, or solutions are that achieve the brief but without successfully using the Spark infrastructure.\n",
        "\n",
        "---\n",
        "\n",
        "In addition, for a grade of E and above, 10 discretionary marks are available for presentation quality of submission (including coding)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347b4bdd",
      "metadata": {
        "id": "347b4bdd"
      },
      "source": [
        "First you need to establish a Spark Session in a slighlty different way using Spark SQL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5999140f",
      "metadata": {
        "id": "5999140f"
      },
      "outputs": [],
      "source": [
        "import pyspark.pandas as ps\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as sql\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916fbc80",
      "metadata": {
        "id": "916fbc80"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Task1\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed29034c",
      "metadata": {
        "id": "ed29034c"
      },
      "outputs": [],
      "source": [
        "df = spark.read.json(\"zdat3002-coursework-1-2025-lilyrobinsn/data/reviews.json\")\n",
        "\n",
        "display(df.limit(5).toPandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02dbce2",
      "metadata": {
        "id": "a02dbce2"
      },
      "source": [
        "Having imported the data, take a look at the schema.  Perhaps try running some SQL queries over it.  I've suggested a first example, but you can come up with more questions.\n",
        "\n",
        "**Can you plot how many ratings of each grade are present in the data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a2aaa6",
      "metadata": {
        "id": "c2a2aaa6"
      },
      "source": [
        "### Hints\n",
        "\n",
        "You've loaded your data, and you want to try and process that data remotely as much as possible, only collecting results at the end.\n",
        "\n",
        "You can add columns to the remote DataFrame using\n",
        "\n",
        "df.withColumn(\"myColumnName\", data)\n",
        "\n",
        "You can execute SQL like operations such as group by and order by:\n",
        "\n",
        "df.orderBy(\"columnName\")\n",
        "df.groupBy(\"columnName\")\n",
        "\n",
        "Think about how you would transform the data in the dataframe, and then collect just the data needed to make the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b78f603",
      "metadata": {
        "id": "1b78f603"
      },
      "outputs": [],
      "source": [
        "rating_counts_spark = df.groupBy(\"overall\").count().orderBy(sql.col(\"overall\").asc())\n",
        "display(rating_counts_spark)\n",
        "\n",
        "rating_counts_pd = rating_counts_spark.toPandas()\n",
        "display(rating_counts_pd)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(rating_counts_pd['overall'].astype(str), rating_counts_pd['count'], color='skyblue')\n",
        "plt.title(\"Distribution of Overall Ratings\")\n",
        "plt.xlabel(\"Rating (Overall Score)\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=0) # Ensure x-axis labels are horizontal\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7) # Add a subtle grid\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d354bac",
      "metadata": {
        "id": "2d354bac"
      },
      "source": [
        "**Can you create a histogram of the number of reviews received on each week of the year.  Are there any patterns present?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c62407",
      "metadata": {
        "id": "54c62407"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"reviewDate\", sql.to_date(sql.to_timestamp(sql.col(\"unixReviewTime\"))))\n",
        "df = df.withColumn(\"weekNumber\", sql.weekofyear(sql.col(\"reviewDate\")))\n",
        "\n",
        "rating_perweek_spark = df.groupBy(\"weekNumber\").count().orderBy(sql.col(\"weekNumber\").asc())\n",
        "\n",
        "rating_perweek_spark = rating_perweek_spark.toPandas()\n",
        "display(rating_perweek_spark)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(rating_perweek_spark['weekNumber'].astype(str), rating_perweek_spark['count'], color='skyblue')\n",
        "plt.title(\"Distribution of Reviews by Week of the Year\")\n",
        "plt.xlabel(\"Review Week\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=0) # Ensure x-axis labels are horizontal\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7) # Add a subtle grid\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a28059e",
      "metadata": {
        "id": "1a28059e"
      },
      "source": [
        "**Can you think of your own query?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd40da72",
      "metadata": {
        "id": "dd40da72"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babaed42",
      "metadata": {
        "id": "babaed42"
      },
      "source": [
        "More tasks will be released at the block release day when you will have time to go deeper, and use much larger datasets on a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15037c5f",
      "metadata": {
        "id": "15037c5f"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}